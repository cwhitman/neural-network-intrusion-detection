import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import tensorflow as tf
import numpy as np
import netaddr
import matplotlib.pyplot as plt
from FTPLogReader.FTPLogReader import FTPLogReader

__author__ = "Caleb Whitman"
__version__ = "1.0.0"
__contributors__ = ["Caleb Whitman", "Brant Dolling", "Jacob Sanderlin"]
__email__ = "calebrwhitman@gmail.com"

"""Returns the connection and centroid tensors.
   Will have to be fed in through a feed dict
   Args:
        nclusters (int): number of clusters.
        log_file (string): FTP Log file.
    Returns: connection_tensor [[ip,average_datetime_difference,ok_login_average,fail_login_average,total_connections ]]"""
def get_FTP_tensors(log_file):
    reader = FTPLogReader(log_file, 0)
    connections = reader.getConnectionTensors()

    return tf.constant(connections,dtype=tf.double)

"""Returns true if the sum of the connections within a centroid drops below a certain value.
A stopping condition is described here :https://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html
Bascially take the sum of distances in each cluster and attempt to get that value below a threshold."""
def should_stop(connection_groups,centroids,threshold,prior_sum):

    #Calculate the sum
    sum = 0
    for i, centroid in enumerate(centroids):
        # Grab just the samples for the given cluster. np.delete removes the IP addresses.
        samples = np.delete(connection_groups[i],0,axis=1)
        distances = np.sum(np.square(samples-centroid),1)

        sum += np.sum(distances)

    if sum < threshold or sum==prior_sum:
        return True, sum
    else:
        return False,sum

"""Creates the random samples for testing purposes."""
def create_samples(n_clusters, n_samples_per_cluster, n_features, embiggen_factor, seed):
    np.random.seed(seed)
    slices = []
    centroids = []
    # Create samples for each cluster
    for i in range(n_clusters):
        #Creates a bunch of random points with normal distribution.
        samples = tf.random_normal((n_samples_per_cluster, n_features),mean=0.0, stddev=5.0, dtype=tf.float32, seed=seed, name="cluster_{}".format(i))
        #Creates random centroid
        current_centroid = (np.random.random((1, n_features)) * embiggen_factor) - (embiggen_factor/2)
        centroids.append(current_centroid)
        #Make samples center around this centroid
        samples += current_centroid
        #centroid is the same as the sample
        slices.append(samples)
    # Create a big "samples" dataset
    samples = tf.concat(slices, 0, name='samples')
    centroids = tf.concat(centroids, 0, name='centroids')
    return centroids, samples


"""Plots average connection time versus total number of connections.
    Args:
        samples ( [[5]] ): The connection vectors.
        centroids ( [ [4] ]): The centroids generated by K_Means
        good_IP_list [ int ]: A list of the good ips.
        bad_IP_list [ int ] : A list of the bad ips."""
def plot_clusters(samples, centroids,good_IP_list,bad_IP_list):

    #Setting up the plot
    fig = plt.figure()
    fig.suptitle('TotalVersusTime', fontsize=14, fontweight='bold')

    ax = fig.add_subplot(111)
    fig.subplots_adjust(top=0.85)
    ax.set_title('Sample 1 Connections')

    ax.set_xlabel('Average Time Between Connections')
    ax.set_ylabel('Total Number of Connections Normalized')

    plt.axis([-100,1000,-100, 1000])

     #Indexes to use in both samples and centroids for plotting.
    x_index = 1
    y_index =-1


     #Finding the perpendicular bisector between the two first centroid values. Bisector stored as two points
    bisector = []
    bisector.append([-100,perpendicular_bisector(centroids[0][x_index-1], centroids[1][x_index-1], centroids[0][y_index],centroids[1][y_index],-100)])
    bisector.append([1500,perpendicular_bisector(centroids[0][x_index-1],centroids[1][x_index-1],centroids[0][y_index],centroids[1][y_index],1500)])

     #Grab just the samples for the given cluster and plot them out with a new colour
    good_samples = [c.tolist() for c in samples if c[0] in good_IP_list ]
    bad_samples = [c.tolist() for c in samples if c[0] in bad_IP_list ]
    good = plt.scatter([s[x_index] for s in good_samples], [s[y_index] for s in good_samples], c="b",label = "Normal Connections")
    bad = plt.scatter([s[x_index] for s in bad_samples], [s[y_index] for s in bad_samples], c="r", label = "Attacks")

     #Also plot centroid
    #plt.plot(centroids[1][x_index-1], centroids[1][y_index], markersize=30, marker="x", color='m', mew=2.5)
    #plt.plot(centroids[0][x_index - 1], centroids[0][y_index], markersize=30, marker="x", color='m', mew=2.5)

    plt.plot([bisector[0][0], bisector[1][0]], [bisector[0][1], bisector[1][1]], color='black', linestyle='-', linewidth=2)

    plt.legend(handles=[good, bad])
    plt.show()

"""Calculates a perpendicular bisector for the given values."""
def perpendicular_bisector(x_a,x_b,y_a,y_b,x):
    slope = (x_a-x_b)/(y_b-y_a)
    x_translation = (x - ((x_a+x_b)/2.0))
    y_translation = (y_a + y_b)/2.0
    return slope * x_translation + y_translation

"""Chooses random centroid from the given samples."""
def choose_random_centroids(samples, n_clusters):

    #Remove the IP addresses from the calculation
    samples = removeIps(samples)

    # Step 0: Initialisation: Select `n_clusters` number of random points
    #Have to subtract one here to ignore IP addresses.
    n_samples = tf.subtract(tf.shape(samples)[0],tf.constant(1,dtype=tf.int32))
    datetimes = samples[:,1]
    total_connections=samples[:,-1]
    datetime_max_index = tf.argmax(datetimes,axis=0)
    total_connections_max_index = tf.argmax(total_connections,axis=0)
    size = [n_clusters,]
    size[0] = n_clusters
    initial_centroids = tf.gather(samples,tf.stack([datetime_max_index,total_connections_max_index]))
    return initial_centroids


"""Finds the nearest centroid for each sample
Returns a list where each element k the index of the nearest centroid in centroid.
For example, [ 0,5,1,1...] indicates taht the first sample is closest to the first centroid, the second sample is closest to the 6th etc."""
def assign_to_nearest(samples, centroids):

    #Remove the IP addresses from the calculation
    samples = removeIps(samples)

    # START from http://esciencegroup.com/2016/01/05/an-encounter-with-googles-tensorflow/
    #Adding a dimension to the vectors so that they can be correctly reduced.
    expanded_vectors = tf.expand_dims(samples, 0)
    expanded_centroids = tf.expand_dims(centroids, 1)

    #Calculating the distances.
    distances = tf.reduce_sum( tf.square(tf.subtract(expanded_vectors, expanded_centroids)), 2)
    mins = tf.argmin(distances, 0)

    # END from http://esciencegroup.com/2016/01/05/an-encounter-with-googles-tensorflow/
    nearest_indices = mins
    return nearest_indices

"""Updates the centroid values based on the nearest indicies."""
def update_centroids(samples, nearest_indices, n_clusters):

    # Updates the centroid to be the mean of all samples associated with it.
    nearest_indices = tf.to_int32(nearest_indices)
    partitions = tf.dynamic_partition(samples, nearest_indices, n_clusters)
    new_centroids = tf.concat([tf.expand_dims(tf.reduce_mean( removeIps(partition), 0), 0) for partition in partitions], 0)
    return new_centroids,partitions

"""Converts the given ip into an interger."""
def turnIptoInt(string_ip):
    return int(netaddr.IPAddress(string_ip))

"""Removes the ip address from the given tensor"""
def removeIps(tensor):
    return tf.slice(tensor,[0,1],[-1,-1])

"""Returns the percentage of good Ips and Bad ips in the connection group.
    Returns: goodIp percentage, badIp percentage"""
def getGoodBadIPCount(connection_groups,index, goodIPs, badIPs):
    goodCount = 0
    badCount = 0
    goodCountTotal=0
    badCountTotal=0
    for ip in connection_groups[index]:
        if (int(ip[0]) in goodIPs):
            goodCount += 1
            goodCountTotal+=1
        else:
            badCount += 1
            badCountTotal+=1

    for ip in connection_groups[index-1]:
        if (int(ip[0]) in goodIPs):
            goodCountTotal+=1
        else:
            badCountTotal+=1

    return goodCount/(goodCountTotal)*100,badCount/(badCountTotal)*100


